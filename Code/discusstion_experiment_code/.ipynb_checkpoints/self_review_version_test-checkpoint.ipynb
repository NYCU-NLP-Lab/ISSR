{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80acd1a9-21a9-4494-bf32-beae20a8693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "比較不同self-review prompt的效果如何\n",
    "discusstion  6-3\n",
    "'''\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class openAIModel():\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(\n",
    "          api_key=\"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\",  # this is also the default, it can be omitted\n",
    "        )\n",
    "        \n",
    "        # This two template has not been used\n",
    "        self.user_template = \"\"\"A good distractor for a multiple-choice question should be carefully crafted to resemble a viable option, making the decision between the correct answer and the distractors more challenging.\n",
    "Distractor's word length should be close to correct answer, and distractor's pos tag should be same with correct answer, and distractor's word frequency should be close to correct answer.\n",
    "A good distractor should not be the antonym of correct answer.\n",
    "The goal is to ensure that test-takers who haven't thoroughly know the meaning of correct answer are more likely to select the distractor.\n",
    "\n",
    "While the distractor should be a plausible choice, it should not fit well in the context when compared to the correct answer.\\n\\n\"\"\"\n",
    "        self.gpt_template = \"\"\"\n",
    "Got it! I'll keep those guidelines in mind. What's the question you need distractors for?\n",
    "\"\"\"\n",
    "        self.chat = list()\n",
    "        self.notify_using_model = False\n",
    "\n",
    "    def inference(self, config):\n",
    "        if config.get(\"temperature\") is not None:\n",
    "            temperature = config['temperature']\n",
    "        else:\n",
    "            temperature = 0.3\n",
    "\n",
    "        if config.get(\"model_name\") is not None:\n",
    "            model = config['model_name']\n",
    "        else:\n",
    "            model = \"gpt-3.5-turbo-1106\"\n",
    "        # Notify user the model they are using    \n",
    "        if not self.notify_using_model:\n",
    "            print(f\"Now inferencing with {model}\")\n",
    "            self.notify_using_model = True\n",
    "        # for degugging: print the prompt sending to model\n",
    "        #self.test_prompt()\n",
    "        \n",
    "        # Call GPT API and get response\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=self.chat,\n",
    "            temperature = temperature,\n",
    "            n=1 # generate one choice only)\n",
    "        )\n",
    "        gpt_response = completion.choices[0].message.content\n",
    "        #print(f\"-----[GPT]-----\\n{gpt_response}\")\n",
    "        self.clear_chat()\n",
    "        return gpt_response\n",
    "\n",
    "    \n",
    "    # clear chat history\n",
    "    def clear_chat(self):\n",
    "        self.chat = []\n",
    "\n",
    "    def append_chat(self, prompt, role):\n",
    "        self.chat.append({\n",
    "            \"role\": role,\n",
    "            \"content\": prompt\n",
    "        })\n",
    "        \n",
    "    def test_prompt(self):\n",
    "        print(self.chat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b390b5-34cd-4711-851b-3a3de3830c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(response, cand_pool = None):\n",
    "    # remove ' \" in response\n",
    "    response = response.replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "    pattern = re.compile(\"\\d+\\. \")\n",
    "    response = [x.lower().replace(\"\\\"\", \"\") for x in pattern.sub(\"\", response).strip().split(\"\\n\") if x != \"\"]\n",
    "    response = list(set(response))\n",
    "    # Further process output by checking whether it exist in english dictionary or not\n",
    "    new_response = []\n",
    "    for i in response:\n",
    "        new_response.append(i.strip().lower())\n",
    "    return new_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71722d1a-7fd0-4e49-8694-3ff897048e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'The newcomer speaks with a strong Irish [MASK]; he must be from Ireland.', 'answer': 'accent', 'distractors': ['identity', 'gratitude', 'signature']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dataset = None\n",
    "with open(\"../dataset/processed_gsat_data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "    # skip the fewshoted question\n",
    "    dataset = dataset[2:]\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57aedac8-8121-4736-a4ad-49eb5dbf47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_good_distractor(dataset, model):\n",
    "    good_distractor = 0\n",
    "    for question in tqdm(dataset):\n",
    "        for d in question['distractors']:\n",
    "            question_prompt = f\"\"\"\n",
    "    Imagine you are a english teacher that designing a vocabulary test to a second language learner, and you came up with a distrctor candidate {{OPTION1}} \n",
    "    \n",
    "    Qustion:\n",
    "    {{STEM}}\n",
    "    \n",
    "    Correct answer:\n",
    "    {{ANSWER}}\n",
    "    \n",
    "    Distractor candidate:\n",
    "    {{OPTION1}}\n",
    "\n",
    "    The criteria for question creation are as follows:\n",
    "    1. The length difference between the answer and the distractor\n",
    "should not exceed 2 characters.\n",
    "    2. The answer and the distractor should share the same part\n",
    "of speech.\n",
    "    3. The difficulty levels between the answer and distractor\n",
    "should be closely matched\n",
    "    \n",
    "    Do you think whether word {{OPTION1}} is a good distractor or not? Response with Yes or No only.\n",
    "    \"\"\"\n",
    "            question_prompt = question_prompt.replace(\"{STEM}\", question['sentence'].replace(\"[MASK]\", \"_____\")).replace(\"{ANSWER}\", question['answer'])\n",
    "            question_prompt = question_prompt.replace(\"{OPTION1}\", d.lower())\n",
    "    \n",
    "            model.append_chat(question_prompt, 'user')\n",
    "            print(question_prompt)\n",
    "            return\n",
    "            response = model.inference({})\n",
    "            if \"yes\" not in response.lower():\n",
    "                pass\n",
    "            else:\n",
    "                good_distractor +=1\n",
    "\n",
    "    print(f\"Good distractor count: {good_distractor} / {len(dataset)*3}\")\n",
    "    return good_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42c29ab1-77e8-4dc0-900e-466b35f841e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_same_meaning(dataset, model):\n",
    "    good_distractor = 0\n",
    "    for question in tqdm(dataset):\n",
    "        prompt = f\"\"\"\n",
    "You will now see two sentences with only one word difference between them:\n",
    "\n",
    "Sentence 1:\n",
    "{{STEM1}}\n",
    "\n",
    "Sentence 2:\n",
    "{{STEM2}}\n",
    "\n",
    "Do these two sentences have the same meaning? Please respond with 'Yes' or 'No' only.\n",
    "\"\"\"\n",
    "        for dis in question['distractors']:\n",
    "            sentence = [question['sentence'].replace(\"[MASK]\", question['answer']), question['sentence'].replace(\"[MASK]\", dis.lower())]\n",
    "            random.shuffle(sentence)\n",
    "            send_prompt = prompt.replace(\"{STEM1}\", sentence[0]).replace(\"{STEM2}\", sentence[1])\n",
    "            model.append_chat(send_prompt, 'user')\n",
    "            print(question_prompt)\n",
    "            return\n",
    "            response = model.inference({}).lower()\n",
    "            if \"no\" not in response.lower():\n",
    "                pass\n",
    "            else:\n",
    "                good_distractor +=1\n",
    "    print(f\"Good distractor count: {good_distractor} / {len(dataset)*3}\")\n",
    "    return good_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b46aefc8-61c8-4712-92ff-f9633b156860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_of_distractr(dataset, model):\n",
    "    good_distractor = 0\n",
    "    both_are_good_list = []\n",
    "    for question in tqdm(dataset):\n",
    "        for d in question['distractors']:\n",
    "            question_prompt = f\"\"\"\n",
    "Imagine you are a high school student that studying english, and you are answering question given below:\n",
    "The following is a vocabulary test that requires selecting one answer from given options to fill in the blank.\n",
    "Please select the option that fit the context best from below, response with the correct option directly, if you think both options are suitable for the context, response with \"BOTH ARE GOOD\".\n",
    "\n",
    "Qustion:\n",
    "{{STEM}}\n",
    "\n",
    "options:\n",
    "{{OPTION1}}\n",
    "{{OPTION2}}\"\"\"\n",
    "            question_prompt = question_prompt.replace(\"{STEM}\", question['sentence'].replace(\"[MASK]\", \"_____\")).replace(\"{OPTION2}\", question['answer'])\n",
    "            question_prompt = question_prompt.replace(\"{OPTION1}\", d.lower())\n",
    "    \n",
    "            model.append_chat(question_prompt, 'user')\n",
    "            response = model.inference({})\n",
    "            if question['answer'].lower() not in response.lower():\n",
    "                if \"both\" in response.lower():\n",
    "                    both_are_good_list.append((ind, response))\n",
    "            else:\n",
    "                good_distractor +=1\n",
    "\n",
    "    print(f\"Good distractor count: {good_distractor} / {len(dataset)*3}\")\n",
    "    return good_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40309a7-bb85-47e7-ae84-386250b2862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "good_distractor = 0\n",
    "\n",
    "dataset = None\n",
    "with open(\"../dataset/processed_gsat_data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "    # skip the fewshoted question\n",
    "    dataset = dataset[2:]\n",
    "\n",
    "result = list()\n",
    "model = openAIModel(\"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\")\n",
    "\n",
    "\n",
    "# test_same_meaning, answer_question_of_distractr, test_good_distractor分別對應3種self-review prompt，\n",
    "answer_question_of_distractr(dataset, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e23617-9ab3-4ebc-829b-4affd7de7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Now inferencing with gpt-3.5-turbo-1106\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "Good distractor count: 3 / 579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "good_distractor = 0\n",
    "\n",
    "dataset = None\n",
    "with open(\"../dataset/processed_gsat_data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "    # skip the fewshoted question\n",
    "    dataset = dataset[2:]\n",
    "\n",
    "result = list()\n",
    "model = openAIModel(\"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\")\n",
    "\n",
    "test_good_distractor(dataset, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FF",
   "language": "python",
   "name": "ff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
