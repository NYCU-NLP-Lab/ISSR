{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the configuration here\n",
    "\"\"\"\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# input your API key here\n",
    "\n",
    "# openAI - for gpt\n",
    "API_KEY = \"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\"\n",
    "\n",
    "# groq - for llama3\n",
    "#API_KEY = \"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\"\n",
    "\n",
    "\n",
    "'''\n",
    "Candidate generator\n",
    "'''\n",
    "preprocess_mode = widgets.Dropdown(\n",
    "    options=['None', 'CDGP', 'BERT'], # in ISSR: CDGP\n",
    "    value='CDGP',\n",
    "    description='Preprocess Mode: Select your candidate generator model',\n",
    ")\n",
    "candidate_generator_top_k = widgets.IntSlider(\n",
    "    value=2500, # in ISSR: 2500\n",
    "    min=100,\n",
    "    max=4000,\n",
    "    step=10,\n",
    "    description='candidate_generator_top_k: How much candidates would be generated by candidate generator (this amount is BEFORE filter, make sure to be big)',\n",
    ")\n",
    "# if this is set to true, BERT model will not be load as candidate generator in runtime (mocked), instead will load contents in \"candidate_set_cache\" as candidate set, not affecting performance\n",
    "use_cache_result = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Use cache result: Use cached candidate set generated by PLM (same stem+target word tends to generate same result)'\n",
    ")\n",
    "# candidate set cache location, used for \"use_cache_result\"\n",
    "# CAUTION: Make sure to select the correct file for caching: *BERT_response_cache* contains results generated by BERT, whereas *CDGP_response_cache* contains results generated by CDGP.\n",
    "candidate_set_cache_path = \"./dataset/BERT_response_cache.json\"\n",
    "\n",
    "\n",
    "cheat = widgets.Checkbox(\n",
    "    value=False, # in ISSR: false\n",
    "    description='''Enable Cheat: Randomly replace some generated candidates to ground truth\n",
    "(to ensure ground truth exists in the candidate set)'''\n",
    ")\n",
    "\n",
    "candidate_set_size = widgets.IntSlider(\n",
    "    value=50, # in ISSR: 50\n",
    "    min=30,\n",
    "    max=300,\n",
    "    step=10,\n",
    "    description='Candidate Set Size: Size of candidate set'\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "Distractor selector\n",
    "'''\n",
    "zero_shot = widgets.Checkbox(\n",
    "    value=False, # in ISSR: false\n",
    "    description='Zero-Shot Mode: Controls distractor selector zero-shot or few shot (true = zeroshot, false=fewshot)'\n",
    ")\n",
    "\n",
    "chain_of_thought = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Chain of Thought: Whether use CoT on distractor selector or not (set to false)' \n",
    ")\n",
    "\n",
    "\n",
    "pick_distractors_per_round = widgets.IntSlider(\n",
    "    value=3, # in ISSR: 3\n",
    "    min=3,\n",
    "    max=30,\n",
    "    step=5,\n",
    "    description='Distractors per Round: Control distractors that picked by distractor selector per round'\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "Self-review\n",
    "'''\n",
    "\n",
    "self_review = widgets.Checkbox(\n",
    "    value=True, # in ISSR: true\n",
    "    description='Self Answer: Using self-review or not'\n",
    ")\n",
    "\n",
    "error_report = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Error Report: Abandoned (set to false)'\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "Overall\n",
    "'''\n",
    "\n",
    "LLM = widgets.Dropdown(\n",
    "    options=['gpt', 'gemma2', 'llama3_8b', 'llama3_70b'],\n",
    "    value='gpt', # in ISSR: gpt\n",
    "    description='''LLM Model: The LLM model used for distractor selector and self-reviewer\n",
    "groq-API / OpenAI api required for models.'''\n",
    ")\n",
    "\n",
    "model_name = widgets.Dropdown(\n",
    "    options=['gpt-4-turbo-2024-04-09', 'gpt-3.5-turbo-0125', 'llama3-70b-8192',\n",
    "             'llama3-8b-8192', 'gpt-4o-mini-2024-07-18'],\n",
    "    value='gpt-3.5-turbo-0125', # in ISSR: gpt-3.5-turbo-0125\n",
    "    description='LLM Model: Specify LLM.'\n",
    ")\n",
    "\n",
    "generate_count = widgets.IntSlider(\n",
    "    value=30, # in ISSR: 30\n",
    "    min=10,\n",
    "    max=100,\n",
    "    step=10,\n",
    "    description='Generate Count: Total required distractors from ISSR'\n",
    ")\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=['cuda', 'cpu'],\n",
    "    value='cuda',\n",
    "    description='Device: your device'\n",
    ")\n",
    "\n",
    "\n",
    "record_bad_distractor = widgets.Checkbox(\n",
    "    value=True, # for recording bad distractors, not affecting performance\n",
    "    description='Record bad distractor: Record select history of distractor selector'\n",
    ")\n",
    "\n",
    "# location of rule-based reference datas\n",
    "# CEEC word list\n",
    "ref_vocabulary_path = \"../Dataset/高中英文參考詞彙表v2.xlsx\"\n",
    "# GSAT questions (or questions required to generate distractors)\n",
    "dataset_path = \"../Dataset/processed_gsat_data.json\"\n",
    "# english dictionary list, which records all english vocabulary (used to test whether the generated candidate is a vocabulary or not)\n",
    "english_dictionary_list_path = \"../Dataset/words_alpha.txt\"\n",
    "\n",
    "\n",
    "print(\"'''HERE IS YOUR CONFIG - MODIFY IT IN CODE, NOT GUI'''\")\n",
    "display(preprocess_mode, cheat, zero_shot, chain_of_thought, candidate_set_size,\n",
    "        pick_distractors_per_round, generate_count, self_review, error_report, LLM, device)\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    config = {\n",
    "        \"preprocess_function\": {\n",
    "            \"mode\": preprocess_mode.value,\n",
    "            \"reason\": \"gpt\", # this is abandoned, let it be\n",
    "            \"cheat\": cheat.value\n",
    "        },\n",
    "        \"distractor_generation_function\": {\n",
    "            \"zero-shot\": zero_shot.value,\n",
    "            \"chain_of_thought\": chain_of_thought.value,\n",
    "            \"candidate_set_size\": candidate_set_size.value,\n",
    "            \"pick_distractors_per_round\": pick_distractors_per_round.value,\n",
    "            \"generate_count\": generate_count.value,\n",
    "        },\n",
    "        \"post_processing_function\": {\n",
    "            \"self-answer\": self_review.value,\n",
    "            \"error-report\": error_report.value,\n",
    "            \"generate_count\": generate_count.value\n",
    "        },\n",
    "        \"api_key\": API_KEY,\n",
    "        \"use_cache_result\": use_cache_result.value,\n",
    "        \"LLM\": LLM.value,\n",
    "        \"model_name\": model_name.value,\n",
    "        \"device\": device.value,\n",
    "        \"ref_vocabulary_path\": ref_vocabulary_path,\n",
    "        \"dataset_path\": dataset_path,\n",
    "        \"english_dictionary_list_path\": english_dictionary_list_path,\n",
    "        \"candidate_set_cache_path\": candidate_set_cache_path,\n",
    "        \"candidate_generator_top_k\": candidate_generator_top_k.value,\n",
    "        \"record_bad_distractor\": True\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from models import openAIModel, gemma2, llama3_8b, llama3_70b\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from utils import *\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from postprocess import self_answer, self_answer_correctness, self_answer_same_meaning\n",
    "import random\n",
    "from openai import OpenAI\n",
    "\n",
    "class DistractorGenerationModel:\n",
    "    def __init__(\n",
    "            self, \n",
    "            config,\n",
    "            preprocess_function,\n",
    "            distractor_generation_function,\n",
    "            post_processing_function,\n",
    "        ):\n",
    "        with open(config['dataset_path'], \"r\") as f:\n",
    "            self.dataset = json.load(f)\n",
    "            # skip the fewshoted question\n",
    "            self.dataset = self.dataset[2:]\n",
    "        self.config = config\n",
    "        self.ref_word = pd.read_excel(config['ref_vocabulary_path'])\n",
    "        # Load words dictionary for distractor generation filter\n",
    "        file_path = config['english_dictionary_list_path']  # Replace \"your_file.txt\" with the path to your text file\n",
    "        self.word_list = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read each line and append it to the list\n",
    "            for line in file:\n",
    "                self.word_list.append(line.strip())  # Strip any leading/trailing whitespace or newline characters\n",
    "        \n",
    "\n",
    "        \n",
    "        # Choose the LLM to inference\n",
    "        if config['LLM'] == \"gpt\":\n",
    "            # Model name is used for LLM templates\n",
    "            self.model = openAIModel(config['api_key'])\n",
    "            self.model_name = \"gpt\"\n",
    "        elif config['LLM'] == \"llama3_8b\":\n",
    "            self.model_name = \"llama3_8b\"\n",
    "            self.model = llama3_8b(config['api_key'])\n",
    "        elif config['LLM'] == \"llama3_70b\":\n",
    "            self.model_name = \"llama3_70b\"\n",
    "            self.model = llama3_70b(config['api_key'])\n",
    "        elif config['LLM'] == \"gemma2\":\n",
    "            self.model_name = \"gemma2\"\n",
    "            self.model = gemma2(config['api_key'])\n",
    "\n",
    "        \n",
    "        self.lemma_model = spacy.load('en_core_web_sm')\n",
    "        # Ready BERT\n",
    "        if (config['preprocess_function']['mode'] == \"BERT\" or config['preprocess_function']['mode'] == \"both\") and config['use_cache_result'] == False:\n",
    "            print(\"Loading BERT as candidate generator...\")\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "            self.bert_csg_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "            self.unmasker = pipeline('fill-mask', tokenizer=self.bert_tokenizer, model=self.bert_csg_model, top_k=config['candidate_generator_top_k'])\n",
    "        elif (config['preprocess_function']['mode'] == \"CDGP\" or config['preprocess_function']['mode'] == \"both\"):\n",
    "            print(\"Loading cdgp-csg-bert as candidate generator...\")\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained(\"AndyChiang/cdgp-csg-bert-cloth\")\n",
    "            self.bert_csg_model = BertForMaskedLM.from_pretrained(\"AndyChiang/cdgp-csg-bert-cloth\")\n",
    "            self.unmasker = pipeline('fill-mask', tokenizer=self.bert_tokenizer, model=self.bert_csg_model, top_k=config['candidate_generator_top_k'])\n",
    "        self.preprocess_function = preprocess_function\n",
    "        self.distractor_generation_function = distractor_generation_function\n",
    "        self.post_processing_function = post_processing_function\n",
    "\n",
    "    # filter list of candidates given predefined rules\n",
    "    def _filter_good_cand(self, cs, question):\n",
    "        filtered_cs = list()\n",
    "        for c in cs:\n",
    "            if self._has_same_postag(c, question) and self._has_sim_length(c, question) and self._has_sim_difficulty(c, question):\n",
    "                filtered_cs.append(c)\n",
    "        return filtered_cs\n",
    "\n",
    "\n",
    "    def _has_sim_length(self, gen_distractor, question):\n",
    "        ans_len = len(question['answer'])\n",
    "        if(ans_len - len(gen_distractor) > 2):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _has_same_postag(self, gen_distractor, question):\n",
    "        \n",
    "        sentence = question['sentence']\n",
    "        answer = question['answer']\n",
    "        ans_pos_tag = get_pos_tag_of_word(sentence, answer)\n",
    "        if(get_pos_tag_of_word(sentence, gen_distractor) != ans_pos_tag):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def _has_sim_difficulty(self, gen_distractor, question):\n",
    "        ref_word = self.ref_word.copy()\n",
    "        ref_word.set_index('單字', inplace=True)\n",
    "        answer = question['answer']\n",
    "        answer = lemmatization(answer, self.lemma_model)\n",
    "        result = ref_word[ref_word.index == answer]\n",
    "        if result.empty:\n",
    "            # print(\"WARNING, answer not in ref word list\")\n",
    "            # print(f\"question: {question['sentence']}\")\n",
    "            return True\n",
    "        else:\n",
    "            ans_dif = result['難度'].values[0]\n",
    "\n",
    "        gen_distractor = lemmatization(gen_distractor, self.lemma_model)\n",
    "        result = ref_word[ref_word.index == gen_distractor]\n",
    "        if result.empty:\n",
    "            return False\n",
    "        else:\n",
    "            if(abs(result['難度'].values[0]-ans_dif) > 1):\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "\n",
    "    def extract_response(self, response, cand_pool = None):\n",
    "        # remove ' \" in response\n",
    "        response = response.replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "        if self.model_name == 'zephyr':\n",
    "          pattern = re.compile(\"\\d+\\. \")\n",
    "          response = [x.lower().replace(\"\\\"\", \"\") for x in pattern.sub(\"\", response).strip().split(\"\\n\") if x != \"\"]\n",
    "        elif self.model_name == 'vicuna-1.5-original':\n",
    "          pattern = re.compile(\"\\d+\\. \")\n",
    "          response = [x.lower().replace(\"\\\"\", \"\") for x in pattern.sub(\"\", response).strip().split(\"\\n\") if x != \"\"]\n",
    "        elif 'gpt' in self.model_name:\n",
    "          pattern = re.compile(\"\\d+\\. \")\n",
    "          response = [x.lower().replace(\"\\\"\", \"\") for x in pattern.sub(\"\", response).strip().split(\"\\n\") if x != \"\"]\n",
    "        else:\n",
    "          pattern = re.compile(\"\\d+\\. \")\n",
    "          response = [x.lower().replace(\"\\\"\", \"\") for x in pattern.sub(\"\", response).strip().split(\"\\n\") if x != \"\"]\n",
    "        response = list(set(response))\n",
    "        # Further process output by checking whether it exist in english dictionary or not\n",
    "        new_response = []\n",
    "        for i in response:\n",
    "            # check if the response vocabulary exists in english dictionary(dataset/words_alpha)\n",
    "            if i.strip().lower() not in self.word_list:\n",
    "                pass\n",
    "            else:\n",
    "                new_response.append(i.strip().lower())\n",
    "                \n",
    "        # Further process output by checking wheher it exist in candidate set\n",
    "        # if cand_pool is not None:\n",
    "        #     cand_pool = [x.lower().strip() for x in cand_pool]\n",
    "        #     final_response=[]\n",
    "        #     for i in new_response:\n",
    "        #         if i.strip().lower() in cand_pool:\n",
    "        #             final_response.append(i.strip())\n",
    "        #         else:\n",
    "        #             pass\n",
    "        #     new_response = final_response\n",
    "        return new_response\n",
    "    \n",
    "    def recall_rate_of_top_k(self, k):\n",
    "        recall_total = list()\n",
    "        for question in self.dataset:\n",
    "            match = 0\n",
    "            pool = self.preprocess_function(self, question)['cand_pool'][:k]\n",
    "            if len(pool) < k:\n",
    "                print(f\"Warning: Valid distractor set size is lower than {k}\")\n",
    "            for d in question['distractors']:\n",
    "                if d in pool:\n",
    "                    match+=1\n",
    "                recall_total.append(match)\n",
    "        return (sum(recall_total)/len(recall_total))/3\n",
    "\n",
    "    def run_framework(self):\n",
    "        self.result = []\n",
    "        self.overall_generate_history = []\n",
    "        self.bert_result = list()\n",
    "\n",
    "\n",
    "        if config['preprocess_function']['mode'] == \"BERT\" and config['use_cache_result'] == True:\n",
    "            with open(config['candidate_set_cache_path'], \"r\") as f:\n",
    "                fastdata = json.load(f)\n",
    "        if config['preprocess_function']['mode'] == \"CDGP\" and config['use_cache_result'] == True:\n",
    "            with open(config['candidate_set_cache_path'], \"r\") as f:\n",
    "                fastdata = json.load(f)\n",
    "        \n",
    "        for ind, question in enumerate(self.dataset):\n",
    "            print(ind)\n",
    "            # The passed distractor\n",
    "            self.good_distractor = []\n",
    "            # The bad distractor in this question\n",
    "            self.persist_bad_distractor = []\n",
    "            # The bad distractor in this round of picking\n",
    "            self.bad_distractor = []\n",
    "            # Do the preprocess part (generation of candidate set using BERT)\n",
    "            if (config['preprocess_function']['mode'] == \"BERT\" or config['preprocess_function']['mode'] == \"CDGP\") and config['use_cache_result'] == True:\n",
    "                prompt_pool = {\n",
    "                    \"reason\": None,\n",
    "                    \"cand_pool\": fastdata[ind]\n",
    "                }\n",
    "                candidate_size = self.config[\"distractor_generation_function\"][\"candidate_set_size\"]\n",
    "                prompt_pool['cand_pool'] = prompt_pool['cand_pool'][:candidate_size]\n",
    "            else:\n",
    "                prompt_pool = self.preprocess_function(self, question)\n",
    "                candidate_size = self.config[\"distractor_generation_function\"][\"candidate_set_size\"]\n",
    "                if(prompt_pool.get('cand_pool') is not None):\n",
    "                    prompt_pool['cand_pool'] = prompt_pool['cand_pool'][:candidate_size]\n",
    "\n",
    "\n",
    "            \n",
    "            # cheat: decrease the size of cand_pool to K which contains the ground truth\n",
    "            if config['preprocess_function']['cheat'] == True:\n",
    "                # Cut the size of cand_pool into K\n",
    "                prompt_pool['cand_pool'] = prompt_pool['cand_pool'][:10]\n",
    "                # randomly replace three distractors candidate to ground truth\n",
    "                numbers_range = list(range(0, min(len(prompt_pool['cand_pool']), 30)))\n",
    "                unique_numbers = random.sample(numbers_range, 3)\n",
    "                ground_truth = question['distractors']\n",
    "                for uni, dis in zip(unique_numbers, ground_truth):\n",
    "                    prompt_pool['cand_pool'][uni] = dis\n",
    "            # Do the distractor generation part\n",
    "            if ind == 0:\n",
    "                # Print prompt\n",
    "                distractor_pool = self.distractor_generation_function(self, question, prompt_pool, sample=True)\n",
    "            else:\n",
    "                distractor_pool = self.distractor_generation_function(self, question, prompt_pool, sample=False)\n",
    "            \n",
    "            \n",
    "            if self.config[\"post_processing_function\"]['self-answer'] == False:\n",
    "                question['generated'] = distractor_pool\n",
    "                continue\n",
    "            \n",
    "            # Self-answer\n",
    "            self.post_processing_function(self, question, distractor_pool)\n",
    "            generate_history = []\n",
    "            t = []\n",
    "            for g in self.good_distractor:\n",
    "                t.append(g)\n",
    "            for b in self.bad_distractor:\n",
    "                t.append(b)\n",
    "            generate_history.append(t)\n",
    "            # Retry to generate distractors for at most 'tries' time\n",
    "            tries = 0\n",
    "            while len(self.good_distractor) < self.config['post_processing_function']['generate_count'] and tries < 2:\n",
    "                tries+=1\n",
    "                if(self.config[\"post_processing_function\"][\"self-answer\"] == True):\n",
    "                    for d in self.bad_distractor:\n",
    "                        if self.config[\"preprocess_function\"]['mode'] != \"None\" and d in prompt_pool['cand_pool']:\n",
    "                            prompt_pool['cand_pool'].remove(d)\n",
    "                    for d in self.good_distractor:\n",
    "                        if self.config[\"preprocess_function\"]['mode'] != \"None\" and d in prompt_pool['cand_pool']:\n",
    "                            prompt_pool['cand_pool'].remove(d)\n",
    "                for d in self.bad_distractor:\n",
    "                    self.persist_bad_distractor.append(d)\n",
    "                    \n",
    "                t = []\n",
    "                for g in self.good_distractor:\n",
    "                    t.append(g)\n",
    "                for b in self.bad_distractor:\n",
    "                    t.append(b)\n",
    "                generate_history.append(t)\n",
    "            \n",
    "                self.bad_distractor = []\n",
    "                distractor_pool = self.distractor_generation_function(self, question, prompt_pool)\n",
    "                self.post_processing_function(self, question, distractor_pool)\n",
    "\n",
    "            # if good distractor is less then 3, append previous bad distractors to the result\n",
    "            self.persist_bad_distractor = list(set(self.persist_bad_distractor))\n",
    "            for d in self.persist_bad_distractor:\n",
    "                if(len(self.good_distractor) < self.config['post_processing_function']['generate_count']):\n",
    "                    self.good_distractor.append(d)\n",
    "                else:\n",
    "                    break\n",
    "            if len(self.good_distractor) < self.config['post_processing_function']['generate_count']:\n",
    "                # The generated distractor is still less then 3\n",
    "                question['generated'] = self.good_distractor\n",
    "            else:\n",
    "                question['generated'] = self.good_distractor[:self.config['post_processing_function']['generate_count']]\n",
    "            self.overall_generate_history.append(generate_history)\n",
    "            generate_history = []\n",
    "        return\n",
    "        #return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from preprocess import *\n",
    "from distractor_generation import *\n",
    "from utils import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = get_config()\n",
    "    print(config)\n",
    "    if(config['preprocess_function']['mode'] == \"BERT\") or config['preprocess_function']['mode'] == \"CDGP\":\n",
    "        preprocess_function = pool_generation\n",
    "    elif(config['preprocess_function']['mode'] == \"reason\"):\n",
    "        preprocess_function = reason_generation\n",
    "    elif(config['preprocess_function']['mode'] == \"None\"):\n",
    "        preprocess_function = none\n",
    "    else:\n",
    "        preprocess_function = pool_generation\n",
    "\n",
    "\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "    \n",
    "    # dis_model = DistractorGenerationModel(config, preprocess_function, few_shot, self_answer)\n",
    "    dis_model = DistractorGenerationModel(config, preprocess_function, few_shot, self_answer)\n",
    "    result = dis_model.run_framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Format: {distractor_selector_model}_{distractors picked per round}_{candidate generator}_{fewshot or zeroshot}_{self-review or not}.json\n",
    "'''\n",
    "result_path = f\"\"\"{config['LLM']}_pickRate{config['distractor_generation_function'][\"pick_distractors_per_round\"]}_{config['preprocess_function']['mode']}_{'fewshot' if config['distractor_generation_function']['zero-shot'] == False else 'zeroshot'}_{'selfanswer' if config['post_processing_function']['self-answer'] == True else 'none'}.json\"\"\"\n",
    "result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = f\"\"\"./result/{config['LLM']}_pickRate{config['distractor_generation_function'][\"pick_distractors_per_round\"]}_{config['preprocess_function']['mode']}_{'fewshot' if config['distractor_generation_function']['zero-shot'] == False else 'zeroshot'}_{'selfanswer' if config['post_processing_function']['self-answer'] == True else 'none'}.json\"\"\"\n",
    "\n",
    "with open(result_path, 'w') as f:\n",
    "    print(f\"writing to {result_path}\")\n",
    "    json.dump(dis_model.dataset, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FF",
   "language": "python",
   "name": "ff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
